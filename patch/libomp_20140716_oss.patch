diff --git a/Makefile b/Makefile
index 40e0327..e6cd784 100644
--- a/Makefile
+++ b/Makefile
@@ -29,6 +29,8 @@
 # </copyright>
 
 omp_root?=.
+tsan?=enabled
+
 include $(omp_root)/tools/common.inc
 .PHONY: default all omp
 
@@ -37,12 +39,12 @@ default: omp
 all: omp stubs
 
 omp: info mkdir
-	@echo $(omp_root)/tools/build.pl $(build_args) --arch=$(arch) --mode=$(mode) lib inc common
-	$(omp_root)/tools/build.pl $(build_args) --arch=$(arch) --mode=$(mode) lib inc common
+	@echo $(omp_root)/tools/build.pl $(build_args) --arch=$(arch) --mode=$(mode) --tsan=$(tsan) lib inc common
+	$(omp_root)/tools/build.pl $(build_args) --arch=$(arch) --mode=$(mode) --tsan=$(tsan) lib inc common
 
 stubs: mkdir
-	@echo $(omp_root)/tools/build.pl $(build_args) --arch=$(arch) --mode=$(mode) --stubs lib inc common 
-	$(omp_root)/tools/build.pl $(build_args) --arch=$(arch) --mode=$(mode) --stubs lib inc common
+	@echo $(omp_root)/tools/build.pl $(build_args) --arch=$(arch) --mode=$(mode) --tsan=$(tsan) --stubs lib inc common 
+	$(omp_root)/tools/build.pl $(build_args) --arch=$(arch) --mode=$(mode) --tsan=$(tsan) --stubs lib inc common
 
 .PHONY: clean info
 
diff --git a/src/dynamic_annotations.h b/src/dynamic_annotations.h
index e69de29..e595d74 100644
--- a/src/dynamic_annotations.h
+++ b/src/dynamic_annotations.h
@@ -0,0 +1,661 @@
+/* Copyright (c) 2011, Google Inc.
+* All rights reserved.
+*
+* Redistribution and use in source and binary forms, with or without
+* modification, are permitted provided that the following conditions are
+* met:
+*
+* * Redistributions of source code must retain the above copyright
+* notice, this list of conditions and the following disclaimer.
+* * Neither the name of Google Inc. nor the names of its
+* contributors may be used to endorse or promote products derived from
+* this software without specific prior written permission.
+*
+* THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+* "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+* LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+* A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+* OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+* SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+* LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+* DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+* THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+* (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+* OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+*/
+
+/* This file defines dynamic annotations for use with dynamic analysis
+tool such as valgrind, PIN, etc.
+
+Dynamic annotation is a source code annotation that affects
+the generated code (that is, the annotation is not a comment).
+Each such annotation is attached to a particular
+instruction and/or to a particular object (address) in the program.
+
+The annotations that should be used by users are macros in all upper-case
+(e.g., ANNOTATE_NEW_MEMORY).
+
+Actual implementation of these macros may differ depending on the
+dynamic analysis tool being used.
+
+See http://code.google.com/p/data-race-test/ for more information.
+
+This file supports the following dynamic analysis tools:
+- None (DYNAMIC_ANNOTATIONS_ENABLED is not defined or zero).
+Macros are defined empty.
+- ThreadSanitizer, Helgrind, DRD (DYNAMIC_ANNOTATIONS_ENABLED is 1).
+Macros are defined as calls to non-inlinable empty functions
+that are intercepted by Valgrind. */
+
+#ifndef __DYNAMIC_ANNOTATIONS_H__
+#define __DYNAMIC_ANNOTATIONS_H__
+
+#ifndef DYNAMIC_ANNOTATIONS_PREFIX
+# define DYNAMIC_ANNOTATIONS_PREFIX
+#endif
+
+// #ifndef DYNAMIC_ANNOTATIONS_PROVIDE_RUNNING_ON_VALGRIND
+// # define DYNAMIC_ANNOTATIONS_PROVIDE_RUNNING_ON_VALGRIND 1
+// #endif
+
+#ifdef DYNAMIC_ANNOTATIONS_WANT_ATTRIBUTE_WEAK
+# ifdef __GNUC__
+# define DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK __attribute__((weak))
+# else
+/* TODO(glider): for Windows support we may want to change this macro in order
+to prepend __declspec(selectany) to the annotations' declarations. */
+# error weak annotations are not supported for your compiler
+# endif
+#else
+# define DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK
+#endif
+
+/* The following preprocessor magic prepends the value of
+DYNAMIC_ANNOTATIONS_PREFIX to annotation function names. */
+#define DYNAMIC_ANNOTATIONS_GLUE0(A, B) A##B
+#define DYNAMIC_ANNOTATIONS_GLUE(A, B) DYNAMIC_ANNOTATIONS_GLUE0(A, B)
+#define DYNAMIC_ANNOTATIONS_NAME(name) \
+DYNAMIC_ANNOTATIONS_GLUE(DYNAMIC_ANNOTATIONS_PREFIX, name)
+
+//#undef DYNAMIC_ANNOTATIONS_ENABLED
+//# define DYNAMIC_ANNOTATIONS_ENABLED 0
+
+#ifndef DYNAMIC_ANNOTATIONS_ENABLED
+# define DYNAMIC_ANNOTATIONS_ENABLED 0
+#endif
+
+#if DYNAMIC_ANNOTATIONS_ENABLED != 0
+
+  /* -------------------------------------------------------------
+Annotations useful when implementing condition variables such as CondVar,
+using conditional critical sections (Await/LockWhen) and when constructing
+user-defined synchronization mechanisms.
+
+The annotations ANNOTATE_HAPPENS_BEFORE() and ANNOTATE_HAPPENS_AFTER() can
+be used to define happens-before arcs in user-defined synchronization
+mechanisms: the race detector will infer an arc from the former to the
+latter when they share the same argument pointer.
+
+Example 1 (reference counting):
+
+void Unref() {
+ANNOTATE_HAPPENS_BEFORE(&refcount_);
+if (AtomicDecrementByOne(&refcount_) == 0) {
+ANNOTATE_HAPPENS_AFTER(&refcount_);
+delete this;
+}
+}
+
+Example 2 (message queue):
+
+void MyQueue::Put(Type *e) {
+MutexLock lock(&mu_);
+ANNOTATE_HAPPENS_BEFORE(e);
+PutElementIntoMyQueue(e);
+}
+
+Type *MyQueue::Get() {
+MutexLock lock(&mu_);
+Type *e = GetElementFromMyQueue();
+ANNOTATE_HAPPENS_AFTER(e);
+return e;
+}
+
+Note: when possible, please use the existing reference counting and message
+queue implementations instead of inventing new ones. */
+
+  /* Report that wait on the condition variable at address "cv" has succeeded
+and the lock at address "lock" is held. */
+#define ANNOTATE_CONDVAR_LOCK_WAIT(cv, lock) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateCondVarWait)(__FILE__, __LINE__, cv, lock)
+
+  /* Report that wait on the condition variable at "cv" has succeeded. Variant
+w/o lock. */
+#define ANNOTATE_CONDVAR_WAIT(cv) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateCondVarWait)(__FILE__, __LINE__, cv, NULL)
+
+  /* Report that we are about to signal on the condition variable at address
+"cv". */
+#define ANNOTATE_CONDVAR_SIGNAL(cv) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateCondVarSignal)(__FILE__, __LINE__, cv)
+
+  /* Report that we are about to signal_all on the condition variable at address
+"cv". */
+#define ANNOTATE_CONDVAR_SIGNAL_ALL(cv) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateCondVarSignalAll)(__FILE__, __LINE__, cv)
+
+  /* Annotations for user-defined synchronization mechanisms. */
+  #define ANNOTATE_HAPPENS_BEFORE(obj) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateHappensBefore)(__FILE__, __LINE__, obj)
+#define ANNOTATE_HAPPENS_AFTER(obj) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateHappensAfter)(__FILE__, __LINE__, obj)
+
+  /* DEPRECATED. Don't use it. */
+  #define ANNOTATE_PUBLISH_MEMORY_RANGE(pointer, size) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotatePublishMemoryRange)(__FILE__, __LINE__, \
+pointer, size)
+
+  /* DEPRECATED. Don't use it. */
+  #define ANNOTATE_UNPUBLISH_MEMORY_RANGE(pointer, size) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateUnpublishMemoryRange)(__FILE__, __LINE__, \
+pointer, size)
+
+  /* DEPRECATED. Don't use it. */
+  #define ANNOTATE_SWAP_MEMORY_RANGE(pointer, size) \
+do { \
+ANNOTATE_UNPUBLISH_MEMORY_RANGE(pointer, size); \
+ANNOTATE_PUBLISH_MEMORY_RANGE(pointer, size); \
+} while (0)
+
+  /* Instruct the tool to create a happens-before arc between mu->Unlock() and
+mu->Lock(). This annotation may slow down the race detector and hide real
+races. Normally it is used only when it would be difficult to annotate each
+of the mutex's critical sections individually using the annotations above.
+This annotation makes sense only for hybrid race detectors. For pure
+happens-before detectors this is a no-op. For more details see
+http://code.google.com/p/data-race-test/wiki/PureHappensBeforeVsHybrid . */
+#define ANNOTATE_PURE_HAPPENS_BEFORE_MUTEX(mu) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateMutexIsUsedAsCondVar)(__FILE__, __LINE__, \
+mu)
+
+  /* Opposite to ANNOTATE_PURE_HAPPENS_BEFORE_MUTEX.
+Instruct the tool to NOT create h-b arcs between Unlock and Lock, even in
+pure happens-before mode. For a hybrid mode this is a no-op. */
+#define ANNOTATE_NOT_HAPPENS_BEFORE_MUTEX(mu) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateMutexIsNotPHB)(__FILE__, __LINE__, mu)
+
+  /* Deprecated. Use ANNOTATE_PURE_HAPPENS_BEFORE_MUTEX. */
+  #define ANNOTATE_MUTEX_IS_USED_AS_CONDVAR(mu) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateMutexIsUsedAsCondVar)(__FILE__, __LINE__, \
+mu)
+
+  /* -------------------------------------------------------------
+Annotations useful when defining memory allocators, or when memory that
+was protected in one way starts to be protected in another. */
+
+  /* Report that a new memory at "address" of size "size" has been allocated.
+This might be used when the memory has been retrieved from a free list and
+is about to be reused, or when a the locking discipline for a variable
+changes. */
+#define ANNOTATE_NEW_MEMORY(address, size) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateNewMemory)(__FILE__, __LINE__, address, \
+size)
+
+  /* -------------------------------------------------------------
+Annotations useful when defining FIFO queues that transfer data between
+threads. */
+
+  /* Report that the producer-consumer queue (such as ProducerConsumerQueue) at
+address "pcq" has been created. The ANNOTATE_PCQ_* annotations
+should be used only for FIFO queues. For non-FIFO queues use
+ANNOTATE_HAPPENS_BEFORE (for put) and ANNOTATE_HAPPENS_AFTER (for get). */
+#define ANNOTATE_PCQ_CREATE(pcq) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotatePCQCreate)(__FILE__, __LINE__, pcq)
+
+  /* Report that the queue at address "pcq" is about to be destroyed. */
+  #define ANNOTATE_PCQ_DESTROY(pcq) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotatePCQDestroy)(__FILE__, __LINE__, pcq)
+
+  /* Report that we are about to put an element into a FIFO queue at address
+"pcq". */
+#define ANNOTATE_PCQ_PUT(pcq) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotatePCQPut)(__FILE__, __LINE__, pcq)
+
+  /* Report that we've just got an element from a FIFO queue at address
+"pcq". */
+#define ANNOTATE_PCQ_GET(pcq) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotatePCQGet)(__FILE__, __LINE__, pcq)
+
+  /* -------------------------------------------------------------
+Annotations that suppress errors. It is usually better to express the
+program's synchronization using the other annotations, but these can
+be used when all else fails. */
+
+  /* Report that we may have a benign race at "pointer", with size
+"sizeof(*(pointer))". "pointer" must be a non-void* pointer. Insert at the
+point where "pointer" has been allocated, preferably close to the point
+where the race happens. See also ANNOTATE_BENIGN_RACE_STATIC. */
+#define ANNOTATE_BENIGN_RACE(pointer, description) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateBenignRaceSized)(__FILE__, __LINE__, \
+pointer, sizeof(*(pointer)), description)
+
+  /* Same as ANNOTATE_BENIGN_RACE(address, description), but applies to
+the memory range [address, address+size). */
+#define ANNOTATE_BENIGN_RACE_SIZED(address, size, description) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateBenignRaceSized)(__FILE__, __LINE__, \
+address, size, description)
+
+  /* Request the analysis tool to ignore all reads in the current thread
+until ANNOTATE_IGNORE_READS_END is called.
+Useful to ignore intentional racey reads, while still checking
+other reads and all writes.
+See also ANNOTATE_UNPROTECTED_READ. */
+#define ANNOTATE_IGNORE_READS_BEGIN() \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateIgnoreReadsBegin)(__FILE__, __LINE__)
+
+  /* Stop ignoring reads. */
+  #define ANNOTATE_IGNORE_READS_END() \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateIgnoreReadsEnd)(__FILE__, __LINE__)
+
+  /* Similar to ANNOTATE_IGNORE_READS_BEGIN, but ignore writes. */
+  #define ANNOTATE_IGNORE_WRITES_BEGIN() \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateIgnoreWritesBegin)(__FILE__, __LINE__)
+
+  /* Stop ignoring writes. */
+  #define ANNOTATE_IGNORE_WRITES_END() \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateIgnoreWritesEnd)(__FILE__, __LINE__)
+
+  /* Start ignoring all memory accesses (reads and writes). */
+  #define ANNOTATE_IGNORE_READS_AND_WRITES_BEGIN() \
+do {\
+ANNOTATE_IGNORE_READS_BEGIN();\
+ANNOTATE_IGNORE_WRITES_BEGIN();\
+}while(0)\
+
+  /* Stop ignoring all memory accesses. */
+  #define ANNOTATE_IGNORE_READS_AND_WRITES_END() \
+do {\
+ANNOTATE_IGNORE_WRITES_END();\
+ANNOTATE_IGNORE_READS_END();\
+}while(0)\
+
+  /* Similar to ANNOTATE_IGNORE_READS_BEGIN, but ignore synchronization events:
+RWLOCK* and CONDVAR*. */
+#define ANNOTATE_IGNORE_SYNC_BEGIN() \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateIgnoreSyncBegin)(__FILE__, __LINE__)
+
+  /* Stop ignoring sync events. */
+  #define ANNOTATE_IGNORE_SYNC_END() \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateIgnoreSyncEnd)(__FILE__, __LINE__)
+
+
+  /* Enable (enable!=0) or disable (enable==0) race detection for all threads.
+This annotation could be useful if you want to skip expensive race analysis
+during some period of program execution, e.g. during initialization. */
+#define ANNOTATE_ENABLE_RACE_DETECTION(enable) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateEnableRaceDetection)(__FILE__, __LINE__, \
+enable)
+
+  /* -------------------------------------------------------------
+Annotations useful for debugging. */
+
+  /* Request to trace every access to "address". */
+  #define ANNOTATE_TRACE_MEMORY(address) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateTraceMemory)(__FILE__, __LINE__, address)
+
+  /* Report the current thread name to a race detector. */
+  #define ANNOTATE_THREAD_NAME(name) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateThreadName)(__FILE__, __LINE__, name)
+
+  /* -------------------------------------------------------------
+Annotations useful when implementing locks. They are not
+normally needed by modules that merely use locks.
+The "lock" argument is a pointer to the lock object. */
+
+  /* Report that a lock has been created at address "lock". */
+  #define ANNOTATE_RWLOCK_CREATE(lock) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateRWLockCreate)(__FILE__, __LINE__, lock)
+
+  /* Report that the lock at address "lock" is about to be destroyed. */
+  #define ANNOTATE_RWLOCK_DESTROY(lock) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateRWLockDestroy)(__FILE__, __LINE__, lock)
+
+  /* Report that the lock at address "lock" has been acquired.
+is_w=1 for writer lock, is_w=0 for reader lock. */
+#define ANNOTATE_RWLOCK_ACQUIRED(lock, is_w) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateRWLockAcquired)(__FILE__, __LINE__, lock, \
+is_w)
+
+  /* Report that the lock at address "lock" is about to be released. */
+  #define ANNOTATE_RWLOCK_RELEASED(lock, is_w) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateRWLockReleased)(__FILE__, __LINE__, lock, \
+is_w)
+
+  /* -------------------------------------------------------------
+Annotations useful when implementing barriers. They are not
+normally needed by modules that merely use barriers.
+The "barrier" argument is a pointer to the barrier object. */
+
+  /* Report that the "barrier" has been initialized with initial "count".
+If 'reinitialization_allowed' is true, initialization is allowed to happen
+multiple times w/o calling barrier_destroy() */
+#define ANNOTATE_BARRIER_INIT(barrier, count, reinitialization_allowed) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateBarrierInit)(__FILE__, __LINE__, barrier, \
+count, reinitialization_allowed)
+
+  /* Report that we are about to enter barrier_wait("barrier"). */
+  #define ANNOTATE_BARRIER_WAIT_BEFORE(barrier) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateBarrierWaitBefore)(__FILE__, __LINE__, \
+barrier)
+
+  /* Report that we just exited barrier_wait("barrier"). */
+  #define ANNOTATE_BARRIER_WAIT_AFTER(barrier) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateBarrierWaitAfter)(__FILE__, __LINE__, \
+barrier)
+
+  /* Report that the "barrier" has been destroyed. */
+  #define ANNOTATE_BARRIER_DESTROY(barrier) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateBarrierDestroy)(__FILE__, __LINE__, \
+barrier)
+
+  /* -------------------------------------------------------------
+Annotations useful for testing race detectors. */
+
+  /* Report that we expect a race on the variable at "address".
+Use only in unit tests for a race detector. */
+#define ANNOTATE_EXPECT_RACE(address, description) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateExpectRace)(__FILE__, __LINE__, address, \
+description)
+
+#define ANNOTATE_FLUSH_EXPECTED_RACES() \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateFlushExpectedRaces)(__FILE__, __LINE__)
+
+  /* A no-op. Insert where you like to test the interceptors. */
+  #define ANNOTATE_NO_OP(arg) \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateNoOp)(__FILE__, __LINE__, arg)
+
+  /* Force the race detector to flush its state. The actual effect depends on
+* the implementation of the detector. */
+#define ANNOTATE_FLUSH_STATE() \
+DYNAMIC_ANNOTATIONS_NAME(AnnotateFlushState)(__FILE__, __LINE__)
+
+
+#else /* DYNAMIC_ANNOTATIONS_ENABLED == 0 */
+
+#define ANNOTATE_RWLOCK_CREATE(lock) /* empty */
+#define ANNOTATE_RWLOCK_DESTROY(lock) /* empty */
+#define ANNOTATE_RWLOCK_ACQUIRED(lock, is_w) /* empty */
+#define ANNOTATE_RWLOCK_RELEASED(lock, is_w) /* empty */
+#define ANNOTATE_BARRIER_INIT(barrier, count, reinitialization_allowed) /* */
+#define ANNOTATE_BARRIER_WAIT_BEFORE(barrier) /* empty */
+#define ANNOTATE_BARRIER_WAIT_AFTER(barrier) /* empty */
+#define ANNOTATE_BARRIER_DESTROY(barrier) /* empty */
+#define ANNOTATE_CONDVAR_LOCK_WAIT(cv, lock) /* empty */
+#define ANNOTATE_CONDVAR_WAIT(cv) /* empty */
+#define ANNOTATE_CONDVAR_SIGNAL(cv) /* empty */
+#define ANNOTATE_CONDVAR_SIGNAL_ALL(cv) /* empty */
+#define ANNOTATE_HAPPENS_BEFORE(obj) /* empty */
+#define ANNOTATE_HAPPENS_AFTER(obj) /* empty */
+#define ANNOTATE_PUBLISH_MEMORY_RANGE(address, size) /* empty */
+#define ANNOTATE_UNPUBLISH_MEMORY_RANGE(address, size) /* empty */
+#define ANNOTATE_SWAP_MEMORY_RANGE(address, size) /* empty */
+#define ANNOTATE_PCQ_CREATE(pcq) /* empty */
+#define ANNOTATE_PCQ_DESTROY(pcq) /* empty */
+#define ANNOTATE_PCQ_PUT(pcq) /* empty */
+#define ANNOTATE_PCQ_GET(pcq) /* empty */
+#define ANNOTATE_NEW_MEMORY(address, size) /* empty */
+#define ANNOTATE_EXPECT_RACE(address, description) /* empty */
+#define ANNOTATE_FLUSH_EXPECTED_RACES(address, description) /* empty */
+#define ANNOTATE_BENIGN_RACE(address, description) /* empty */
+#define ANNOTATE_BENIGN_RACE_SIZED(address, size, description) /* empty */
+#define ANNOTATE_PURE_HAPPENS_BEFORE_MUTEX(mu) /* empty */
+#define ANNOTATE_MUTEX_IS_USED_AS_CONDVAR(mu) /* empty */
+#define ANNOTATE_TRACE_MEMORY(arg) /* empty */
+#define ANNOTATE_THREAD_NAME(name) /* empty */
+#define ANNOTATE_IGNORE_READS_BEGIN() /* empty */
+#define ANNOTATE_IGNORE_READS_END() /* empty */
+#define ANNOTATE_IGNORE_WRITES_BEGIN() /* empty */
+#define ANNOTATE_IGNORE_WRITES_END() /* empty */
+#define ANNOTATE_IGNORE_READS_AND_WRITES_BEGIN() /* empty */
+#define ANNOTATE_IGNORE_READS_AND_WRITES_END() /* empty */
+#define ANNOTATE_IGNORE_SYNC_BEGIN() /* empty */
+#define ANNOTATE_IGNORE_SYNC_END() /* empty */
+#define ANNOTATE_ENABLE_RACE_DETECTION(enable) /* empty */
+#define ANNOTATE_NO_OP(arg) /* empty */
+#define ANNOTATE_FLUSH_STATE() /* empty */
+
+#endif /* DYNAMIC_ANNOTATIONS_ENABLED */
+
+//#undef ANNOTATE_HAPPENS_BEFORE
+//#undef ANNOTATE_HAPPENS_AFTER
+//#define ANNOTATE_HAPPENS_BEFORE(obj) /* empty */
+//#define ANNOTATE_HAPPENS_AFTER(obj) /* empty */
+
+/* specify the locks to be instrumented */
+#define ANNOTATE_QUEUING
+ #define ANNOTATE_TICKET
+#define ANNOTATE_FUTEX
+ #define ANNOTATE_TAS
+#define ANNOTATE_DRDPA
+
+#ifdef ANNOTATE_QUEUING
+#define ANNOTATE_QUEUING_CREATE(lck) ANNOTATE_RWLOCK_CREATE((void *) lck)
+#define ANNOTATE_QUEUING_RELEASED(lck) ANNOTATE_RWLOCK_RELEASED((void *) lck, 1)
+#define ANNOTATE_QUEUING_ACQUIRED(lck) ANNOTATE_RWLOCK_ACQUIRED((void *) lck, 1)
+#else
+#define ANNOTATE_QUEUING_CREATE(lck) (void)0
+#define ANNOTATE_QUEUING_RELEASED(lck) (void)0
+#define ANNOTATE_QUEUING_ACQUIRED(lck) (void)0
+#endif
+
+#ifdef ANNOTATE_TICKET
+#define ANNOTATE_TICKET_CREATE(lck) ANNOTATE_RWLOCK_CREATE((void *) lck)
+#define ANNOTATE_TICKET_RELEASED(lck) ANNOTATE_RWLOCK_RELEASED((void *) lck, 1)
+#define ANNOTATE_TICKET_ACQUIRED(lck) ANNOTATE_RWLOCK_ACQUIRED((void *) lck, 1)
+#else
+#define ANNOTATE_TICKET_CREATE(lck) (void)0
+#define ANNOTATE_TICKET_RELEASED(lck) (void)0
+#define ANNOTATE_TICKET_ACQUIRED(lck) (void)0
+#endif
+
+#ifdef ANNOTATE_FUTEX
+#define ANNOTATE_FUTEX_CREATE(lck) ANNOTATE_RWLOCK_CREATE((void *) lck)
+#define ANNOTATE_FUTEX_RELEASED(lck) ANNOTATE_RWLOCK_RELEASED((void *) lck, 1)
+#define ANNOTATE_FUTEX_ACQUIRED(lck) ANNOTATE_RWLOCK_ACQUIRED((void *) lck, 1)
+#else
+#define ANNOTATE_FUTEX_CREATE(lck) (void)0
+#define ANNOTATE_FUTEX_RELEASED(lck) (void)0
+#define ANNOTATE_FUTEX_ACQUIRED(lck) (void)0
+#endif
+
+#ifdef ANNOTATE_TAS
+#define ANNOTATE_TAS_CREATE(lck) ANNOTATE_RWLOCK_CREATE((void *) lck)
+#define ANNOTATE_TAS_RELEASED(lck) ANNOTATE_RWLOCK_RELEASED((void *) lck, 1)
+#define ANNOTATE_TAS_ACQUIRED(lck) ANNOTATE_RWLOCK_ACQUIRED((void *) lck, 1)
+#else
+#define ANNOTATE_TAS_CREATE(lck) (void)0
+#define ANNOTATE_TAS_RELEASED(lck) (void)0
+#define ANNOTATE_TAS_ACQUIRED(lck) (void)0
+#endif
+
+#ifdef ANNOTATE_DRDPA
+#define ANNOTATE_DRDPA_CREATE(lck) ANNOTATE_RWLOCK_CREATE((void *) lck)
+#define ANNOTATE_DRDPA_RELEASED(lck) ANNOTATE_RWLOCK_RELEASED((void *) lck, 1)
+#define ANNOTATE_DRDPA_ACQUIRED(lck) ANNOTATE_RWLOCK_ACQUIRED((void *) lck, 1)
+#else
+#define ANNOTATE_DRDPA_CREATE(lck) (void)0
+#define ANNOTATE_DRDPA_RELEASED(lck) (void)0
+#define ANNOTATE_DRDPA_ACQUIRED(lck) (void)0
+#endif
+
+
+/* Use the macros above rather than using these functions directly. */
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateRWLockCreate)(
+    const char *file, int line,
+    const volatile void *lock) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateRWLockDestroy)(
+    const char *file, int line,
+    const volatile void *lock) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateRWLockAcquired)(
+    const char *file, int line,
+    const volatile void *lock, long is_w) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateRWLockReleased)(
+    const char *file, int line,
+    const volatile void *lock, long is_w) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateBarrierInit)(
+    const char *file, int line, const volatile void *barrier, long count,
+    long reinitialization_allowed) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateBarrierWaitBefore)(
+    const char *file, int line,
+    const volatile void *barrier) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateBarrierWaitAfter)(
+    const char *file, int line,
+    const volatile void *barrier) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateBarrierDestroy)(
+    const char *file, int line,
+    const volatile void *barrier) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateCondVarWait)(
+    const char *file, int line, const volatile void *cv,
+    const volatile void *lock) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateCondVarSignal)(
+    const char *file, int line,
+    const volatile void *cv) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateCondVarSignalAll)(
+    const char *file, int line,
+    const volatile void *cv) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateHappensBefore)(
+    const char *file, int line,
+    const volatile void *obj) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateHappensAfter)(
+    const char *file, int line,
+    const volatile void *obj) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotatePublishMemoryRange)(
+    const char *file, int line,
+    const volatile void *address, long size) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateUnpublishMemoryRange)(
+    const char *file, int line,
+    const volatile void *address, long size) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotatePCQCreate)(
+    const char *file, int line,
+    const volatile void *pcq) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotatePCQDestroy)(
+    const char *file, int line,
+    const volatile void *pcq) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotatePCQPut)(
+    const char *file, int line,
+    const volatile void *pcq) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotatePCQGet)(
+    const char *file, int line,
+    const volatile void *pcq) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateNewMemory)(
+    const char *file, int line,
+    const volatile void *mem, long size) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateExpectRace)(
+    const char *file, int line, const volatile void *mem,
+    const char *description) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateFlushExpectedRaces)(
+    const char *file, int line) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateBenignRace)(
+    const char *file, int line, const volatile void *mem,
+    const char *description) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateBenignRaceSized)(
+    const char *file, int line, const volatile void *mem, long size,
+    const char *description) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateMutexIsUsedAsCondVar)(
+    const char *file, int line,
+    const volatile void *mu) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateMutexIsNotPHB)(
+    const char *file, int line,
+    const volatile void *mu) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateTraceMemory)(
+    const char *file, int line,
+    const volatile void *arg) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateThreadName)(
+    const char *file, int line,
+    const char *name) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateIgnoreReadsBegin)(
+    const char *file, int line) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateIgnoreReadsEnd)(
+    const char *file, int line) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateIgnoreWritesBegin)(
+    const char *file, int line) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateIgnoreWritesEnd)(
+    const char *file, int line) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateIgnoreSyncBegin)(
+    const char *file, int line) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateIgnoreSyncEnd)(
+    const char *file, int line) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateEnableRaceDetection)(
+    const char *file, int line, int enable) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateNoOp)(
+    const char *file, int line,
+    const volatile void *arg) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+void DYNAMIC_ANNOTATIONS_NAME(AnnotateFlushState)(
+    const char *file, int line) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+
+#if DYNAMIC_ANNOTATIONS_PROVIDE_RUNNING_ON_VALGRIND == 1
+/* Return non-zero value if running under valgrind.
+
+If "valgrind.h" is included into dynamic_annotations.c,
+the regular valgrind mechanism will be used.
+See http://valgrind.org/docs/manual/manual-core-adv.html about
+RUNNING_ON_VALGRIND and other valgrind "client requests".
+The file "valgrind.h" may be obtained by doing
+svn co svn://svn.valgrind.org/valgrind/trunk/include
+
+If for some reason you can't use "valgrind.h" or want to fake valgrind,
+there are two ways to make this function return non-zero:
+- Use environment variable: export RUNNING_ON_VALGRIND=1
+- Make your tool intercept the function RunningOnValgrind() and
+change its return value.
+*/
+int RunningOnValgrind(void) DYNAMIC_ANNOTATIONS_ATTRIBUTE_WEAK;
+#endif /* DYNAMIC_ANNOTATIONS_PROVIDE_RUNNING_ON_VALGRIND == 1 */
+
+#ifdef __cplusplus
+}
+#endif
+
+#if DYNAMIC_ANNOTATIONS_ENABLED != 0 && defined(__cplusplus)
+
+  /* ANNOTATE_UNPROTECTED_READ is the preferred way to annotate racey reads.
+
+Instead of doing
+ANNOTATE_IGNORE_READS_BEGIN();
+... = x;
+ANNOTATE_IGNORE_READS_END();
+one can use
+... = ANNOTATE_UNPROTECTED_READ(x); */
+  template <class T>
+  inline T ANNOTATE_UNPROTECTED_READ(const volatile T &x) {
+    ANNOTATE_IGNORE_READS_BEGIN();
+    T res = x;
+    ANNOTATE_IGNORE_READS_END();
+    return res;
+  }
+  /* Apply ANNOTATE_BENIGN_RACE_SIZED to a static variable. */
+  #define ANNOTATE_BENIGN_RACE_STATIC(static_var, description) \
+namespace { \
+class static_var ## _annotator { \
+public: \
+static_var ## _annotator() { \
+ANNOTATE_BENIGN_RACE_SIZED(&static_var, \
+sizeof(static_var), \
+# static_var ": " description); \
+} \
+}; \
+static static_var ## _annotator the ## static_var ## _annotator;\
+}
+#else /* DYNAMIC_ANNOTATIONS_ENABLED == 0 */
+
+#define ANNOTATE_UNPROTECTED_READ(x) (x)
+#define ANNOTATE_BENIGN_RACE_STATIC(static_var, description) /* empty */
+
+#endif /* DYNAMIC_ANNOTATIONS_ENABLED */
+
+#endif /* __DYNAMIC_ANNOTATIONS_H__ */
diff --git a/src/kmp_barrier.cpp b/src/kmp_barrier.cpp
index ac6f459..47600d8 100644
--- a/src/kmp_barrier.cpp
+++ b/src/kmp_barrier.cpp
@@ -56,6 +56,8 @@
 #define ngo_sync()               ((void)0)
 #endif /* KMP_MIC && USE_NGO_STORES */
 
+#include "dynamic_annotations.h"
+
 #ifndef KMP_DEBUG
 #define __kmp_static_delay(arg)  // nothing to do
 #else
@@ -479,6 +481,7 @@ __kmp_linear_barrier_gather(enum barrier_type bt, kmp_info_t *this_thr, int gtid
         // Mark arrival to master thread
         /* After performing this write, a worker thread may not assume that the team is valid
            any more - it could be deallocated by the master thread at any time. */
+        ANNOTATE_HAPPENS_BEFORE(this_thr);
         __kmp_release(other_threads[0], (kmp_uint32 *)&thr_bar->b_arrived);
     } else {
         register kmp_balign_team_t *team_bar = &team->t.t_bar[bt];
@@ -503,6 +506,7 @@ __kmp_linear_barrier_gather(enum barrier_type bt, kmp_info_t *this_thr, int gtid
             __kmp_wait_sleep_64(this_thr, &other_threads[i]->th.th_bar[bt].bb.b_arrived,
                              new_state, FALSE
                              USE_ITT_BUILD_ARG(itt_sync_obj) );
+            ANNOTATE_HAPPENS_AFTER(other_threads[ i ]);
 #if USE_ITT_BUILD && USE_ITT_NOTIFY
             // Barrier imbalance - write min of the thread time and the other thread time to the thread.
             if (__kmp_forkjoin_frames_mode == 2 || __kmp_forkjoin_frames_mode == 3) {
@@ -575,6 +579,7 @@ __kmp_linear_barrier_release(enum barrier_type bt, kmp_info_t *this_thr, int gti
                               &other_threads[i]->th.th_bar[bt].bb.b_go,
                               other_threads[i]->th.th_bar[bt].bb.b_go,
                               other_threads[i]->th.th_bar[bt].bb.b_go + KMP_BARRIER_STATE_BUMP));
+                ANNOTATE_HAPPENS_BEFORE(other_threads[i]);
                 __kmp_release(other_threads[i], (kmp_uint32 *)&other_threads[i]->th.th_bar[bt].bb.b_go);
             }
         }
@@ -584,6 +589,7 @@ __kmp_linear_barrier_release(enum barrier_type bt, kmp_info_t *this_thr, int gti
         __kmp_wait_sleep_64(this_thr, &thr_bar->b_go, KMP_BARRIER_STATE_BUMP, TRUE
                          USE_ITT_BUILD_ARG(itt_sync_obj)
                          );
+        ANNOTATE_HAPPENS_AFTER(this_thr);
 
 #if USE_ITT_BUILD && USE_ITT_NOTIFY
         if ((__itt_sync_create_ptr && itt_sync_obj == NULL) || KMP_ITT_DEBUG) {
@@ -667,6 +673,7 @@ __kmp_tree_barrier_gather(enum barrier_type bt, kmp_info_t *this_thr, int gtid,
             // Wait for child to arrive
             __kmp_wait_sleep_64(this_thr, &child_bar->b_arrived, new_state, FALSE
                              USE_ITT_BUILD_ARG(itt_sync_obj) );
+            ANNOTATE_HAPPENS_AFTER(child_thr);
 #if USE_ITT_BUILD && USE_ITT_NOTIFY
             // Barrier imbalance - write min of the thread time and a child time to the thread.
             if (__kmp_forkjoin_frames_mode == 2 || __kmp_forkjoin_frames_mode == 3) {
@@ -698,6 +705,7 @@ __kmp_tree_barrier_gather(enum barrier_type bt, kmp_info_t *this_thr, int gtid,
         // Mark arrival to parent thread
         /* After performing this write, a worker thread may not assume that the team is valid
            any more - it could be deallocated by the master thread at any time.  */
+        ANNOTATE_HAPPENS_BEFORE(this_thr);
         __kmp_release(other_threads[parent_tid], (kmp_uint32 *)&thr_bar->b_arrived);
     } else {
         // Need to update the team arrived pointer if we are the master thread
@@ -734,6 +742,7 @@ __kmp_tree_barrier_release(enum barrier_type bt, kmp_info_t *this_thr, int gtid,
         // Wait for parent thread to release us
         __kmp_wait_sleep_64(this_thr, &thr_bar->b_go, KMP_BARRIER_STATE_BUMP, TRUE
                          USE_ITT_BUILD_ARG(itt_sync_obj) );
+        ANNOTATE_HAPPENS_AFTER(this_thr);
 
 #if USE_ITT_BUILD && USE_ITT_NOTIFY
         if ((__itt_sync_create_ptr && itt_sync_obj == NULL) || KMP_ITT_DEBUG) {
@@ -802,6 +811,7 @@ __kmp_tree_barrier_release(enum barrier_type bt, kmp_info_t *this_thr, int gtid,
                           child_tid, &child_bar->b_go, child_bar->b_go,
                           child_bar->b_go + KMP_BARRIER_STATE_BUMP));
             // Release child from barrier
+            ANNOTATE_HAPPENS_BEFORE(child_thr);
             __kmp_release(child_thr, (kmp_uint32 *)&child_bar->b_go);
             child++;
             child_tid++;
@@ -860,6 +870,7 @@ __kmp_hyper_barrier_gather(enum barrier_type bt, kmp_info_t *this_thr, int gtid,
             /* After performing this write (in the last iteration of the enclosing for loop),
                a worker thread may not assume that the team is valid any more - it could be
                deallocated by the master thread at any time.  */
+            ANNOTATE_HAPPENS_BEFORE(this_thr);
             __kmp_release(other_threads[parent_tid], (kmp_uint32 *)&thr_bar->b_arrived);
             break;
         }
@@ -885,6 +896,7 @@ __kmp_hyper_barrier_gather(enum barrier_type bt, kmp_info_t *this_thr, int gtid,
             // Wait for child to arrive
             __kmp_wait_sleep_64(this_thr, &child_bar->b_arrived, new_state, FALSE
                              USE_ITT_BUILD_ARG(itt_sync_obj) );
+            ANNOTATE_HAPPENS_AFTER(child_thr);
 #if USE_ITT_BUILD && USE_ITT_NOTIFY
             // Barrier imbalance - write min of the thread time and a child time to the thread.
             if (__kmp_forkjoin_frames_mode == 2 || __kmp_forkjoin_frames_mode == 3) {
@@ -954,6 +966,7 @@ __kmp_hyper_barrier_release(enum barrier_type bt, kmp_info_t *this_thr, int gtid
         // Wait for parent thread to release us
         __kmp_wait_sleep_64(this_thr, &thr_bar->b_go, KMP_BARRIER_STATE_BUMP, TRUE
                          USE_ITT_BUILD_ARG(itt_sync_obj) );
+        ANNOTATE_HAPPENS_AFTER(this_thr);
 
 #if USE_ITT_BUILD && USE_ITT_NOTIFY
         if ((__itt_sync_create_ptr && itt_sync_obj == NULL) || KMP_ITT_DEBUG) {
@@ -1042,6 +1055,7 @@ __kmp_hyper_barrier_release(enum barrier_type bt, kmp_info_t *this_thr, int gtid
                               child_tid, &child_bar->b_go, child_bar->b_go,
                               child_bar->b_go + KMP_BARRIER_STATE_BUMP));
                 // Release child from barrier
+                ANNOTATE_HAPPENS_BEFORE(child_thr);
                 __kmp_release(child_thr, (kmp_uint32 *)&child_bar->b_go);
             }
         }
@@ -1155,9 +1169,17 @@ __kmp_hierarchical_barrier_gather(enum barrier_type bt, kmp_info_t *this_thr,
                         KA_TRACE(100, ("__kmp_hierarchical_barrier_gather: T#%d(%d:%d) += T#%d(%d:%d)\n",
                                        gtid, team->t.t_id, tid, __kmp_gtid_from_tid(child_tid, team),
                                        team->t.t_id, child_tid));
+                        ANNOTATE_HAPPENS_AFTER(other_threads[child_tid]);
                         (*reduce)(this_thr->th.th_local.reduce_data, other_threads[child_tid]->th.th_local.reduce_data);
                     }
                 }
+#ifdef DYN
+                else{
+                    for (child_tid=tid+1; child_tid<=tid+thr_bar->leaf_kids; ++child_tid) {
+                        ANNOTATE_HAPPENS_AFTER(other_threads[child_tid]);
+                    }
+                }
+#endif
                 (void) KMP_TEST_THEN_AND64((volatile kmp_int64 *)&thr_bar->b_arrived, ~(thr_bar->leaf_state)); // clear leaf_state bits
             }
             // Next, wait for higher level children on each child's b_arrived flag
@@ -1173,6 +1195,7 @@ __kmp_hierarchical_barrier_gather(enum barrier_type bt, kmp_info_t *this_thr,
                                   team->t.t_id, child_tid, &child_bar->b_arrived, new_state));
                     __kmp_wait_sleep_64(this_thr, &child_bar->b_arrived, new_state, FALSE
                                         USE_ITT_BUILD_ARG(itt_sync_obj) );
+                    ANNOTATE_HAPPENS_AFTER(child_thr);
                     if (reduce) {
                         KA_TRACE(100, ("__kmp_hierarchical_barrier_gather: T#%d(%d:%d) += T#%d(%d:%d)\n",
                                        gtid, team->t.t_id, tid, __kmp_gtid_from_tid(child_tid, team),
@@ -1195,6 +1218,7 @@ __kmp_hierarchical_barrier_gather(enum barrier_type bt, kmp_info_t *this_thr,
                                   team->t.t_id, child_tid, &child_bar->b_arrived, new_state));
                     __kmp_wait_sleep_64(this_thr, &child_bar->b_arrived, new_state, FALSE
                                         USE_ITT_BUILD_ARG(itt_sync_obj) );
+                    ANNOTATE_HAPPENS_AFTER(child_thr);
                     if (reduce) {
                         KA_TRACE(100, ("__kmp_hierarchical_barrier_gather: T#%d(%d:%d) += T#%d(%d:%d)\n",
                                        gtid, team->t.t_id, tid, __kmp_gtid_from_tid(child_tid, team),
@@ -1216,6 +1240,7 @@ __kmp_hierarchical_barrier_gather(enum barrier_type bt, kmp_info_t *this_thr,
            the team is valid any more - it could be deallocated by the master thread at any time. */
         if (thr_bar->my_level || __kmp_dflt_blocktime != KMP_MAX_BLOCKTIME
             || !thr_bar->use_oncore_barrier) { // Parent is waiting on my b_arrived flag; release it
+            ANNOTATE_HAPPENS_BEFORE(this_thr);
             __kmp_release(other_threads[thr_bar->parent_tid], (kmp_uint32 *)&thr_bar->b_arrived);
         }
         else { // Leaf does special release on the "offset" bits of parent's b_arrived flag
@@ -1257,6 +1282,7 @@ __kmp_hierarchical_barrier_release(enum barrier_type bt, kmp_info_t *this_thr, i
             thr_bar->wait_flag = KMP_BARRIER_OWN_FLAG;
             __kmp_wait_sleep_64(this_thr, &thr_bar->b_go, KMP_BARRIER_STATE_BUMP, TRUE
                              USE_ITT_BUILD_ARG(itt_sync_obj) );
+            ANNOTATE_HAPPENS_AFTER(this_thr);
             TCW_8(thr_bar->b_go, KMP_INIT_BARRIER_STATE); // Reset my b_go flag for next time
         }
         else { // Thread barrier data is initialized, this is a leaf, blocktime is infinite, not nested
@@ -1361,6 +1387,7 @@ __kmp_hierarchical_barrier_release(enum barrier_type bt, kmp_info_t *this_thr, i
                                       team->t.t_id, child_tid, &child_bar->b_go, child_bar->b_go,
                                       child_bar->b_go + KMP_BARRIER_STATE_BUMP));
                         // Release child using child's b_go flag
+                        ANNOTATE_HAPPENS_BEFORE(child_thr);
                         __kmp_release(child_thr, (kmp_uint32 *)&child_bar->b_go);
                     }
                 }
@@ -1383,6 +1410,7 @@ __kmp_hierarchical_barrier_release(enum barrier_type bt, kmp_info_t *this_thr, i
                                   team->t.t_id, child_tid, &child_bar->b_go, child_bar->b_go,
                                   child_bar->b_go + KMP_BARRIER_STATE_BUMP));
                     // Release child using child's b_go flag
+                    ANNOTATE_HAPPENS_BEFORE(child_thr);
                     __kmp_release(child_thr, (kmp_uint32 *)&child_bar->b_go);
                 }
             }
diff --git a/src/kmp_lock.cpp b/src/kmp_lock.cpp
index 8410068..634224e 100644
--- a/src/kmp_lock.cpp
+++ b/src/kmp_lock.cpp
@@ -42,6 +42,8 @@
 #include "kmp_lock.h"
 #include "kmp_io.h"
 
+#include "dynamic_annotations.h"
+
 #if KMP_OS_LINUX && (KMP_ARCH_X86 || KMP_ARCH_X86_64 || KMP_ARCH_ARM)
 # include <unistd.h>
 # include <sys/syscall.h>
@@ -177,6 +179,7 @@ void
 __kmp_acquire_tas_lock( kmp_tas_lock_t *lck, kmp_int32 gtid )
 {
     __kmp_acquire_tas_lock_timed_template( lck, gtid );
+    ANNOTATE_TAS_ACQUIRED(lck);
 }
 
 static void
@@ -221,6 +224,7 @@ __kmp_release_tas_lock( kmp_tas_lock_t *lck, kmp_int32 gtid )
     KMP_MB();       /* Flush all pending memory write invalidates.  */
 
     KMP_FSYNC_RELEASING(lck);
+    ANNOTATE_TAS_RELEASED(lck);
     KMP_ST_REL32( &(lck->lk.poll), 0 );
 
     KMP_MB();       /* Flush all pending memory write invalidates.  */
@@ -295,6 +299,7 @@ __kmp_acquire_nested_tas_lock( kmp_tas_lock_t *lck, kmp_int32 gtid )
     }
     else {
         __kmp_acquire_tas_lock_timed_template( lck, gtid );
+        ANNOTATE_TAS_ACQUIRED(lck);
         lck->lk.depth_locked = 1;
     }
 }
@@ -504,6 +509,7 @@ void
 __kmp_acquire_futex_lock( kmp_futex_lock_t *lck, kmp_int32 gtid )
 {
     __kmp_acquire_futex_lock_timed_template( lck, gtid );
+    ANNOTATE_FUTEX_ACQUIRED(lck);
 }
 
 static void
@@ -550,6 +556,7 @@ __kmp_release_futex_lock( kmp_futex_lock_t *lck, kmp_int32 gtid )
       lck, lck->lk.poll, gtid ) );
 
     KMP_FSYNC_RELEASING(lck);
+    ANNOTATE_FUTEX_RELEASED(lck);
 
     kmp_int32 poll_val = KMP_XCHG_FIXED32( & ( lck->lk.poll ), 0 );
 
@@ -637,6 +644,7 @@ __kmp_acquire_nested_futex_lock( kmp_futex_lock_t *lck, kmp_int32 gtid )
     }
     else {
         __kmp_acquire_futex_lock_timed_template( lck, gtid );
+        ANNOTATE_FUTEX_ACQUIRED(lck);
         lck->lk.depth_locked = 1;
     }
 }
@@ -800,6 +808,7 @@ void
 __kmp_acquire_ticket_lock( kmp_ticket_lock_t *lck, kmp_int32 gtid )
 {
     __kmp_acquire_ticket_lock_timed_template( lck, gtid );
+    ANNOTATE_TICKET_ACQUIRED(lck);
 }
 
 static void
@@ -863,6 +872,7 @@ __kmp_release_ticket_lock( kmp_ticket_lock_t *lck, kmp_int32 gtid )
     KMP_MB();       /* Flush all pending memory write invalidates.  */
 
     KMP_FSYNC_RELEASING(lck);
+    ANNOTATE_TICKET_RELEASED(lck);
     distance = ( TCR_4( lck->lk.next_ticket ) - TCR_4( lck->lk.now_serving ) );
 
     KMP_ST_REL32( &(lck->lk.now_serving), lck->lk.now_serving + 1 );
@@ -954,6 +964,7 @@ __kmp_acquire_nested_ticket_lock( kmp_ticket_lock_t *lck, kmp_int32 gtid )
     }
     else {
         __kmp_acquire_ticket_lock_timed_template( lck, gtid );
+        ANNOTATE_TICKET_ACQUIRED(lck);
         KMP_MB();
         lck->lk.depth_locked = 1;
         KMP_MB();
@@ -1406,6 +1417,7 @@ __kmp_acquire_queuing_lock( kmp_queuing_lock_t *lck, kmp_int32 gtid )
     KMP_DEBUG_ASSERT( gtid >= 0 );
 
     __kmp_acquire_queuing_lock_timed_template<false>( lck, gtid );
+    ANNOTATE_QUEUING_ACQUIRED(lck);
 }
 
 static void
@@ -1454,6 +1466,7 @@ __kmp_test_queuing_lock( kmp_queuing_lock_t *lck, kmp_int32 gtid )
         if ( KMP_COMPARE_AND_STORE_ACQ32( head_id_p, 0, -1 ) ) {
             KA_TRACE( 1000, ("__kmp_test_queuing_lock: T#%d exiting: holding lock\n", gtid ));
             KMP_FSYNC_ACQUIRED(lck);
+            ANNOTATE_QUEUING_ACQUIRED(lck);
             return TRUE;
         }
     }
@@ -1504,6 +1517,7 @@ __kmp_release_queuing_lock( kmp_queuing_lock_t *lck, kmp_int32 gtid )
     KMP_DEBUG_ASSERT( this_thr->th.th_next_waiting == 0 );
 
     KMP_FSYNC_RELEASING(lck);
+    ANNOTATE_QUEUING_RELEASED(lck);
 
     while( 1 ) {
         kmp_int32 dequeued;
@@ -1701,6 +1715,7 @@ __kmp_acquire_nested_queuing_lock( kmp_queuing_lock_t *lck, kmp_int32 gtid )
     }
     else {
         __kmp_acquire_queuing_lock_timed_template<false>( lck, gtid );
+	ANNOTATE_QUEUING_ACQUIRED(lck);
         KMP_MB();
         lck->lk.depth_locked = 1;
         KMP_MB();
@@ -2340,6 +2355,7 @@ __kmp_acquire_adaptive_lock( kmp_adaptive_lock_t * lck, kmp_int32 gtid )
     __kmp_acquire_queuing_lock_timed_template<FALSE>( GET_QLK_PTR(lck), gtid );
     // We have acquired the base lock, so count that.
     KMP_INC_STAT(lck,nonSpeculativeAcquires );
+    ANNOTATE_QUEUING_ACQUIRED(lck);
 }
 
 static void
@@ -2628,6 +2644,7 @@ void
 __kmp_acquire_drdpa_lock( kmp_drdpa_lock_t *lck, kmp_int32 gtid )
 {
     __kmp_acquire_drdpa_lock_timed_template( lck, gtid );
+    ANNOTATE_DRDPA_ACQUIRED(lck);
 }
 
 static void
@@ -2720,6 +2737,7 @@ __kmp_release_drdpa_lock( kmp_drdpa_lock_t *lck, kmp_int32 gtid )
     KA_TRACE(1000, ("__kmp_release_drdpa_lock: ticket #%lld released lock %p\n",
        ticket - 1, lck));
     KMP_FSYNC_RELEASING(lck);
+    ANNOTATE_DRDPA_RELEASED(lck);
     KMP_ST_REL64(&(polls[ticket & mask].poll), ticket); // volatile store
 }
 
@@ -2823,6 +2841,7 @@ __kmp_acquire_nested_drdpa_lock( kmp_drdpa_lock_t *lck, kmp_int32 gtid )
     }
     else {
         __kmp_acquire_drdpa_lock_timed_template( lck, gtid );
+        ANNOTATE_DRDPA_ACQUIRED(lck);
         KMP_MB();
         lck->lk.depth_locked = 1;
         KMP_MB();
@@ -3364,12 +3383,15 @@ __kmp_user_lock_allocate( void **user_lock, kmp_int32 gtid,
 
     if ( __kmp_lock_pool == NULL ) {
         // Lock pool is empty. Allocate new memory.
+        /* ANNOTATION: Found no good way to express the syncronisation between allocation and usage, so ignore the allocation */
+        ANNOTATE_IGNORE_WRITES_BEGIN();
         if ( __kmp_num_locks_in_block <= 1 ) { // Tune this cutoff point.
             lck = (kmp_user_lock_p) __kmp_allocate( __kmp_user_lock_size );
         }
         else {
             lck = __kmp_lock_block_allocate();
         }
+        ANNOTATE_IGNORE_WRITES_END();
 
         // Insert lock in the table so that it can be freed in __kmp_cleanup,
         // and debugger has info on all allocated locks.
diff --git a/src/kmp_runtime.c b/src/kmp_runtime.c
index 4e18cca..61db737 100644
--- a/src/kmp_runtime.c
+++ b/src/kmp_runtime.c
@@ -54,6 +54,7 @@
 #include <process.h>
 #endif
 
+#include "dynamic_annotations.h"
 
 #if defined(KMP_GOMP_COMPAT)
 char const __kmp_version_alt_comp[] = KMP_VERSION_PREFIX "alternative compiler support: yes";
@@ -5181,6 +5182,7 @@ __kmp_reap_thread(
             /* Assume the threads are at the fork barrier here */
             KA_TRACE( 20, ("__kmp_reap_thread: releasing T#%d from fork barrier for reap\n", gtid ) );
             /* Need release fence here to prevent seg faults for tree forkjoin barrier (GEH) */
+            ANNOTATE_HAPPENS_BEFORE(thread);
             __kmp_release(thread, (kmp_uint32 *)&thread->th.th_bar[ bs_forkjoin_barrier ].bb.b_go);
         }; // if
 
diff --git a/src/kmp_tasking.c b/src/kmp_tasking.c
index 748c34e..eb9e61c 100644
--- a/src/kmp_tasking.c
+++ b/src/kmp_tasking.c
@@ -38,6 +38,7 @@
 #include "kmp_i18n.h"
 #include "kmp_itt.h"
 
+#include "dynamic_annotations.h"
 
 
 /* ------------------------------------------------------------------------ */
@@ -537,6 +538,7 @@ __kmp_free_task( kmp_int32 gtid, kmp_taskdata_t * taskdata, kmp_info_t * thread
     KMP_DEBUG_ASSERT( TCR_4(taskdata->td_incomplete_child_tasks) == 0 );
 
     taskdata->td_flags.freed = 1;
+    ANNOTATE_HAPPENS_BEFORE(taskdata);
     // deallocate the taskdata and shared variable blocks associated with this task
     #if USE_FAST_MEMORY
         __kmp_fast_free( thread, taskdata );
@@ -863,6 +865,7 @@ __kmp_task_alloc( ident_t *loc_ref, kmp_int32 gtid, kmp_tasking_flags_t *flags,
     #else /* ! USE_FAST_MEMORY */
     taskdata = (kmp_taskdata_t *) __kmp_thread_malloc( thread, shareds_offset + sizeof_shareds );
     #endif /* USE_FAST_MEMORY */
+    ANNOTATE_HAPPENS_AFTER(taskdata);
 
     task                      = KMP_TASKDATA_TO_TASK(taskdata);
 
@@ -947,6 +950,7 @@ __kmp_task_alloc( ident_t *loc_ref, kmp_int32 gtid, kmp_tasking_flags_t *flags,
     KA_TRACE(20, ("__kmp_task_alloc(exit): T#%d created task %p parent=%p\n",
                   gtid, taskdata, taskdata->td_parent) );
 
+    ANNOTATE_HAPPENS_BEFORE(task);
     return task;
 }
 
@@ -992,6 +996,7 @@ __kmp_invoke_task( kmp_int32 gtid, kmp_task_t *task, kmp_taskdata_t * current_ta
     KA_TRACE(30, ("__kmp_invoke_task(enter): T#%d invoking task %p, current_task=%p\n",
                   gtid, taskdata, current_task) );
 
+    ANNOTATE_HAPPENS_AFTER(task);
     __kmp_task_start( gtid, task, current_task );
 
 #if OMP_40_ENABLED
@@ -1027,6 +1032,7 @@ __kmp_invoke_task( kmp_int32 gtid, kmp_task_t *task, kmp_taskdata_t * current_ta
     }
 #endif // OMP_40_ENABLED
 
+    ANNOTATE_HAPPENS_BEFORE(taskdata->td_parent);
     __kmp_task_finish( gtid, task, current_task );
 
     KA_TRACE(30, ("__kmp_inovke_task(exit): T#%d completed task %p, resuming task %p\n",
@@ -1066,6 +1072,7 @@ __kmpc_omp_task_parts( ident_t *loc_ref, kmp_int32 gtid, kmp_task_t * new_task)
                   "loc=%p task=%p, return: TASK_CURRENT_NOT_QUEUED\n", gtid, loc_ref,
                   new_taskdata ) );
 
+    ANNOTATE_HAPPENS_BEFORE(new_task);
     return TASK_CURRENT_NOT_QUEUED;
 }
 
@@ -1095,6 +1102,7 @@ __kmp_omp_task( kmp_int32 gtid, kmp_task_t * new_task, bool serialize_immediate
     }
 
 
+    ANNOTATE_HAPPENS_BEFORE(new_task);
     return TASK_CURRENT_NOT_QUEUED;
 }
 
@@ -1177,6 +1185,7 @@ __kmpc_omp_taskwait( ident_t *loc_ref, kmp_int32 gtid )
     KA_TRACE(10, ("__kmpc_omp_taskwait(exit): T#%d task %p finished waiting, "
                   "returning TASK_CURRENT_NOT_QUEUED\n", gtid, taskdata) );
 
+    ANNOTATE_HAPPENS_AFTER(taskdata);
     return TASK_CURRENT_NOT_QUEUED;
 }
 
@@ -1302,6 +1311,7 @@ __kmpc_end_taskgroup( ident_t* loc, int gtid )
     __kmp_thread_free( thread, taskgroup );
 
     KA_TRACE(10, ("__kmpc_end_taskgroup(exit): T#%d task %p finished waiting\n", gtid, taskdata) );
+    ANNOTATE_HAPPENS_AFTER(taskdata);
 }
 #endif
 
@@ -2064,8 +2074,10 @@ __kmp_realloc_task_threads_data( kmp_info_t *thread, kmp_task_team_t *task_team
                 // Make the initial allocate for threads_data array, and zero entries
                 // Cannot use __kmp_thread_calloc() because threads not around for
                 // kmp_reap_task_team( ).
+                ANNOTATE_IGNORE_WRITES_BEGIN();
                 *threads_data_p = (kmp_thread_data_t *)
                                   __kmp_allocate( nthreads * sizeof(kmp_thread_data_t) );
+                ANNOTATE_IGNORE_WRITES_END();
 #ifdef BUILD_TIED_TASK_STACK
                 // GEH: Figure out if this is the right thing to do
                 for (i = 0; i < nthreads; i++) {
diff --git a/src/makefile.mk b/src/makefile.mk
index 4d39707..aad6330 100644
--- a/src/makefile.mk
+++ b/src/makefile.mk
@@ -116,6 +116,7 @@ define curr_config
     CXXFLAGS=$(subst $(space),_,$(CXXFLAGS))
     FFLAGS=$(subst $(space),_,$(FFLAGS))
     LDFLAGS=$(subst $(space),_,$(LDFLAGS))
+    TSAN=$(tsan)
 endef
 # And check it.
 include $(tools_dir)src/common-checks.mk
@@ -678,6 +679,9 @@ ld-flags   += $(LDFLAGS)
 # --------------------------------------------------------------------------------------------------
 # Files.
 # --------------------------------------------------------------------------------------------------
+ifeq "$(tsan)" "enabled"
+  cpp-flags += -D DYNAMIC_ANNOTATIONS_ENABLED=1
+endif
 
 # Library files. These files participate in all kinds of library.
 lib_c_items :=      \
@@ -809,6 +813,9 @@ else
             _lib_item += mt
         endif
     endif
+    ifeq "$(tsan)" "enabled"
+	_lib_item += _tsan
+    endif
 endif
 # _lib_item is a list of space separated name parts. Remove spaces to form final name.
 lib_item = $(subst $(space),,$(_lib_item))
@@ -1300,12 +1307,16 @@ ifneq "$(os)" "lrb"
             endif
         else # lin
             ifeq "$(std_cpp_lib)" "1"
-                tt-c        = g++
+                tt-c        = $(cxx)
             else
-                tt-c        = gcc
+                tt-c        = $(c)
             endif
             # GCC on OS X* does not recognize -pthread.
             tt-c-flags  += -pthread
+            ifeq "$(tsan)" "enabled"
+                tt-c-flags  += -fsanitize=thread
+            endif
+
         endif
         tt-c-flags += -o $(tt-exe-file)
         ifneq "$(filter 32 32e 64,$(arch))" ""
diff --git a/src/z_Linux_util.c b/src/z_Linux_util.c
index dfb1492..2719274 100644
--- a/src/z_Linux_util.c
+++ b/src/z_Linux_util.c
@@ -52,6 +52,8 @@
 #include <sys/resource.h>
 #include <sys/syscall.h>
 
+#include "dynamic_annotations.h"
+
 #if KMP_OS_LINUX
 # include <sys/sysinfo.h>
 # if KMP_OS_LINUX && (KMP_ARCH_X86 || KMP_ARCH_X86_64 || KMP_ARCH_ARM)
@@ -1639,6 +1641,7 @@ __kmp_suspend_initialize( void )
 static void
 __kmp_suspend_initialize_thread( kmp_info_t *th )
 {
+    ANNOTATE_HAPPENS_AFTER(&th->th.th_suspend_init_count);
     if ( th->th.th_suspend_init_count <= __kmp_fork_count ) {
         /* this means we haven't initialized the suspension pthread objects for this thread
            in this instance of the process */
@@ -1648,6 +1651,7 @@ __kmp_suspend_initialize_thread( kmp_info_t *th )
         status = pthread_mutex_init( &th->th.th_suspend_mx.m_mutex, & __kmp_suspend_mutex_attr );
         KMP_CHECK_SYSFAIL( "pthread_mutex_init", status );
         *(volatile int*)&th->th.th_suspend_init_count = __kmp_fork_count + 1;
+    ANNOTATE_HAPPENS_BEFORE(&th->th.th_suspend_init_count);
     };
 }
 
diff --git a/tools/build.pl b/tools/build.pl
index e16c4f3..7ab619c 100755
--- a/tools/build.pl
+++ b/tools/build.pl
@@ -88,12 +88,13 @@ my $opts = {
     "mic-arch"        => { targets => "rtl",               base => 0, parms => { knf     => "*", knc       => "", knl => ""   }, suffix => sub { $_[ 0 ];                       } },
     "mic-os"          => { targets => "rtl",               base => 0, parms => { bsd     => "*", lin       => ""              }, suffix => sub { $_[ 0 ];                       } },
     "mic-comp"        => { targets => "rtl",               base => 0, parms => { native  => "*", offload   => ""              }, suffix => sub { substr( $_[ 0 ], 0, 3 );       } },
+    "tsan"    => { targets => "rtl",         base => 0, parms => { enabled => "", disabled => "*"               }, suffix => sub { "" } },
 };
 my $synonyms = {
     "debug" => [ qw{ dbg debg } ],
 };
 # This array specifies order of options to process, so it cannot be initialized with keys( %$opts ).
-my @all_opts   = qw{ target version lib-type link-type target-compiler mode omp-version coverage tcheck mic-arch mic-os mic-comp };
+my @all_opts   = qw{ target version lib-type link-type target-compiler mode omp-version coverage tcheck mic-arch mic-os mic-comp tsan};
 # This is the list of base options.
 my @base_opts  = grep( $opts->{ $_ }->{ base } == 1, @all_opts );
 # This is the list of extra options.
@@ -297,6 +298,7 @@ sub enqueue_jobs($$@) {
                     "VERSION=" . $set->{ version },
                     "TARGET_COMPILER=" . $set->{ "target-compiler" },
                     "suffix=" . $suf,
+                     "tsan=" . $set->{ "tsan" },
                     @goals,
                 ],
                 build_dir  => $build_dir
